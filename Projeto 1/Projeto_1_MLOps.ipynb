{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wUEFZSvBtxy"
      },
      "source": [
        "# Problem definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny19c5SLHXRn"
      },
      "source": [
        "## Airbnb Price Prediction with Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tSCVExKIDvO"
      },
      "source": [
        "\n",
        "**Airbnb** is a marketplace for short-term rentals that allows hosts to list part or all of their living space for others to book. The platform includes everything from private rooms to entire houses. Thanks to its flexibility and competitive prices, Airbnb has become one of the main alternatives to hotels worldwide. Since its founding in 2008, the company has grown exponentially: it went public in December 2020 with a valuation of about **\\$47 billion**, and as of 2025 its market capitalization exceeds **\\$85 billion**, making it more valuable than most global hotel chains.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBELVAshig6_"
      },
      "source": [
        "\n",
        "### The Pricing Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUx7QibliiI2"
      },
      "source": [
        "One of the biggest challenges for hosts is setting the optimal nightly price. In many cities, travelers are presented with a wide variety of listings and can filter by criteria such as:\n",
        "\n",
        "- price  \n",
        "- number of bedrooms  \n",
        "- property type  \n",
        "- location  \n",
        "\n",
        "Because Airbnb is a competitive marketplace, the amount a host can charge is directly tied to supply and demand dynamics.  \n",
        "\n",
        "- If a host sets a price much higher than the local average, guests will likely choose cheaper but similar alternatives.  \n",
        "- If the price is set too low, the host loses potential revenue.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFjYs5jiioVr"
      },
      "source": [
        "\n",
        "### A Simple Strategy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wifULLNrip0V"
      },
      "source": [
        "\n",
        "A straightforward approach to pricing could be:  \n",
        "\n",
        "1. Identify similar listings.  \n",
        "2. Compute the average price among them.  \n",
        "3. Use that value as a reference price.  \n",
        "\n",
        "While simple, this strategy does not fully capture the complexity of the marketplace.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2rk8OhZiuUt"
      },
      "source": [
        "\n",
        "\n",
        "### Machine Learning for Price Prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_OKr5odizKn"
      },
      "source": [
        "\n",
        "The process of using existing data to predict future outcomes is called machine learning. In our case, we want to use local listing data to predict the ideal nightly rate.  \n",
        "\n",
        "In this lesson, we will explore a **regularized linear regression model** (implemented in the [mlops repository](https://github.com/ivanovitchm/mlops)).  \n",
        "\n",
        "This model fits a mathematical function to the available data, capturing relationships between multiple features—such as number of bedrooms, location, and property type—and estimating their impact on the final price. Unlike simple averaging strategies, regression provides a more systematic way to generalize and make robust predictions across different scenarios.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh1k_G7cjCqL"
      },
      "source": [
        "\n",
        "### Next Step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAZN3yEnjDrG"
      },
      "source": [
        "\n",
        "Before diving deeper into the model, let’s first become familiar with the dataset we’ll be working with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkhhRIDa_iDx"
      },
      "source": [
        "# Introduction to the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EptZJnfPkWW9"
      },
      "source": [
        "While AirBnB does not release official data on the listings available in their marketplace, an independent initiative called [Inside AirBnB](https://insideairbnb.com/rio-de-janeiro/) has extracted and published data on a sample of listings for many major cities featured on the platform.  \n",
        "\n",
        "In this lesson, we will work with their dataset from **March 2025**, focusing on the listings from **Rio de Janeiro, Brazil**, the world-renowned capital of Samba. Here is a [direct link to the dataset](https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2025-03-19/data/listings.csv.gz). Each row in this dataset corresponds to a specific property available for short-term rental on AirBnB in Rio de Janeiro.\n",
        "\n",
        "To make the dataset easier to manage, we have removed many of the original 100+ columns. Below are the selected columns we will keep for our analysis:\n",
        "\n",
        "- **host_response_rate**: the response rate of the host  \n",
        "- **host_acceptance_rate**: percentage of requests to the host that convert into rentals  \n",
        "- **host_listings_count**: number of other listings managed by the host  \n",
        "- **latitude**: latitude of the property’s geographic coordinates  \n",
        "- **longitude**: longitude of the property’s geographic coordinates  \n",
        "- **city**: the city where the property is located  \n",
        "- **zipcode**: the postal code of the property  \n",
        "- **state**: the state where the property is located  \n",
        "- **accommodates**: the number of guests the property can host  \n",
        "- **room_type**: the type of accommodation (Private room, Shared room, or Entire home/apt)  \n",
        "- **bedrooms**: number of bedrooms included in the rental  \n",
        "- **bathrooms**: number of bathrooms available  \n",
        "- **beds**: number of beds included  \n",
        "- **price**: nightly rental price  \n",
        "- **cleaning_fee**: additional fee charged for cleaning after the guest’s stay  \n",
        "- **security_deposit**: refundable security deposit in case of damages  \n",
        "- **minimum_nights**: minimum number of nights required for a booking  \n",
        "- **maximum_nights**: maximum number of nights allowed for a booking  \n",
        "- **number_of_reviews**: total number of reviews left by past guests  \n",
        "\n",
        "Let’s load this dataset into Pandas and begin exploring it in more detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y583MCO_7Nj4",
        "outputId": "b31abb3c-286c-458c-eebd-c643dd4fb14e"
      },
      "outputs": [],
      "source": [
        "# Use wget to download the dataset\n",
        "!wget -O listings.csv.gz \"https://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2025-03-19/data/listings.csv.gz\"\n",
        "\n",
        "# Unzip the .gz file (-k keeps the original .gz file)\n",
        "!gunzip -k listings.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "fJ4w_4pHdm8P",
        "outputId": "19a2d581-3483-4c72-f8ab-48449f89b20b"
      },
      "outputs": [],
      "source": [
        "# Now you have the \"listings.csv\" file available in Colab\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df = pd.read_csv(\"listings.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpMjeGIV-nBZ",
        "outputId": "ac58eedb-7baa-4b39-c4f0-13d7e00748ab"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "sS-TwVw5sDik",
        "outputId": "14c8f0ab-34b2-4e6d-c706-3eaaef87d6c7"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKa0y8tObMqA",
        "outputId": "1ecfcb9a-d4f1-4320-819f-cc3989af9b57"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIgI641-cDen",
        "outputId": "8b4ad28c-5382-46bb-9f83-7019196ffd77"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IgcQjAc_JaM"
      },
      "source": [
        "# Clean, Prepare & Manipulate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d8akhh2j_JDQ"
      },
      "outputs": [],
      "source": [
        "# Remove commas from the 'price' column (e.g., \"$1,200\" → \"$1200\")\n",
        "stripped_commas = df['price'].str.replace(',', '')\n",
        "\n",
        "# Remove dollar signs from the 'price' column (e.g., \"$1200\" → \"1200\")\n",
        "stripped_dollars = stripped_commas.str.replace('$', '')\n",
        "\n",
        "# Convert the cleaned 'price' column from string to float type\n",
        "df['price'] = stripped_dollars.astype('float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ybn5nwpM_dTg"
      },
      "outputs": [],
      "source": [
        "# Remove any rows containing missing values across the selected columns\n",
        "#df.dropna(axis=0, inplace=True)\n",
        "df.dropna(subset=['price'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWAlTGzfEHKy",
        "outputId": "68800885-96c4-4149-a8f2-b955c97b7f19"
      },
      "outputs": [],
      "source": [
        "numeric_df = df.select_dtypes(include=[\"number\", \"float\", \"int\"])\n",
        "numeric_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWJmHLoCF4wX",
        "outputId": "7f7d2f6f-78fa-443d-8320-0532c19fe55d"
      },
      "outputs": [],
      "source": [
        "numeric_df = numeric_df.drop(columns=[\n",
        "    \"license\",\n",
        "    \"calendar_updated\",\n",
        "    \"neighbourhood_group_cleansed\",\n",
        "    \"id\",\n",
        "    \"scrape_id\",\n",
        "    \"host_id\",\n",
        "    \"host_listings_count\",\n",
        "    \"host_total_listings_count\"\n",
        "])\n",
        "numeric_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUaoWj0eGTVM",
        "outputId": "3ca44e48-f8fc-44c7-a111-4614c6211f88"
      },
      "outputs": [],
      "source": [
        "numeric_df = numeric_df.dropna(axis=0)\n",
        "numeric_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KT_IKIJPGiHx",
        "outputId": "20d253f9-540c-4b2a-9463-b50e0def3bd4"
      },
      "outputs": [],
      "source": [
        "numeric_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IQF4YH-1EX1-",
        "outputId": "4223624b-fa97-491d-e6d8-104525c6ee20"
      },
      "outputs": [],
      "source": [
        "corr_matrix = numeric_df.corr(method=\"pearson\")\n",
        "corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urlzjwr9_2fv",
        "outputId": "3694edad-91a8-41c8-c5e4-c48ad65626a3"
      },
      "outputs": [],
      "source": [
        "corr_with_price = corr_matrix[\"price\"].sort_values(ascending=False)\n",
        "print(corr_with_price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TKfZj-kKv9t9",
        "outputId": "c48bdf89-4193-4cea-b724-32d850ca25a6"
      },
      "outputs": [],
      "source": [
        "# Select only the relevant columns from the original dataframe 'df'\n",
        "# These columns include property features (e.g., accommodates, bathrooms, bedrooms)\n",
        "# and booking information (e.g., minimum_nights, maximum_nights, number_of_reviews, price).\n",
        "\n",
        "target_columns = [\n",
        "    \"accommodates\",      # Number of guests the property can host\n",
        "    \"bathrooms\",         # Number of bathrooms available\n",
        "    \"bedrooms\",          # Number of bedrooms available\n",
        "    \"beds\",              # Number of beds available\n",
        "    \"minimum_nights\",    # Minimum nights required for a booking\n",
        "    \"review_scores_value\",\n",
        "    \"estimated_occupancy_l365d\",\n",
        "    \"number_of_reviews\", # Total reviews given by past guests\n",
        "    \"price\"              # Nightly rental price\n",
        "]\n",
        "\n",
        "# Create a new dataframe called 'rio_listings' with only the selected columns\n",
        "# Using .copy() ensures that we are working with an independent dataframe\n",
        "# instead of just a view of the original data.\n",
        "\n",
        "rio_listings = df[target_columns].copy()\n",
        "\n",
        "# Display a five rows sample of the new dataframe to quickly check the data\n",
        "rio_listings.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "wMrCBxbP_zPh",
        "outputId": "a34efc41-a2c3-4686-feb0-4ec4dc2b9235"
      },
      "outputs": [],
      "source": [
        "# Generate descriptive statistics\n",
        "rio_listings.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgB8FreM_va_",
        "outputId": "f544342c-b4b2-42e7-e0b2-057f736a0caf"
      },
      "outputs": [],
      "source": [
        "# Check missing values\n",
        "rio_listings.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqCs0kUyBO2c"
      },
      "source": [
        "## Robust IQR-based outlier filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qMt_QD1lBc9B"
      },
      "outputs": [],
      "source": [
        "def _iqr_bounds(series: pd.Series, k: float = 1.5):\n",
        "    \"\"\"\n",
        "    Compute lower/upper bounds for outlier detection using the IQR rule.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        Numeric series.\n",
        "    k : float, default 1.5\n",
        "        Whisker length multiplier (1.5 = Tukey; 3.0 = more tolerant).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    low : float\n",
        "        Lower bound (Q1 - k * IQR).\n",
        "    up : float\n",
        "        Upper bound (Q3 + k * IQR).\n",
        "    stats : dict\n",
        "        Dict with Q1, Q3, IQR.\n",
        "    \"\"\"\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    low = q1 - k * iqr\n",
        "    up = q3 + k * iqr\n",
        "    return low, up, {\"Q1\": q1, \"Q3\": q3, \"IQR\": iqr}\n",
        "\n",
        "\n",
        "def remove_outliers_iqr(\n",
        "    df: pd.DataFrame,\n",
        "    columns: list[str],\n",
        "    k: float = 1.5,\n",
        "    inclusive: bool = True,\n",
        "    dropna: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Remove rows with outliers per-column using the IQR rule.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input DataFrame.\n",
        "    columns : list of str\n",
        "        Columns to evaluate for outliers (must be numeric or coercible).\n",
        "    k : float, default 1.5\n",
        "        Whisker length multiplier (1.5 = standard; 3.0 = lenient).\n",
        "    inclusive : bool, default True\n",
        "        If True, keep values exactly on the bounds; otherwise use strict inequality.\n",
        "    dropna : bool, default True\n",
        "        If True, drop rows with NaN in the selected columns prior to filtering.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cleaned : pd.DataFrame\n",
        "        DataFrame with outlier rows removed.\n",
        "    info : dict\n",
        "        Summary with input/output row counts and per-column bounds/statistics.\n",
        "    \"\"\"\n",
        "    data = df.copy()\n",
        "\n",
        "    # Ensure columns are numeric; non-numeric values become NaN\n",
        "    for c in columns:\n",
        "        data[c] = pd.to_numeric(data[c], errors=\"coerce\")\n",
        "\n",
        "    # Optionally drop NaNs first to avoid excluding entire rows by comparison\n",
        "    if dropna:\n",
        "        data = data.dropna(subset=columns)\n",
        "\n",
        "    before = len(data)\n",
        "    bounds = {}\n",
        "    # Start with all rows valid; refine with each column's mask\n",
        "    mask = pd.Series(True, index=data.index)\n",
        "\n",
        "    for c in columns:\n",
        "        low, up, stats = _iqr_bounds(data[c].dropna(), k=k)\n",
        "        bounds[c] = {\"low\": low, \"up\": up, **stats}\n",
        "\n",
        "        if inclusive:\n",
        "            m = (data[c] >= low) & (data[c] <= up)\n",
        "        else:\n",
        "            m = (data[c] > low) & (data[c] < up)\n",
        "\n",
        "        mask &= m\n",
        "\n",
        "    cleaned = data.loc[mask].copy()\n",
        "    info = {\n",
        "        \"rows_in\": before,\n",
        "        \"rows_out\": len(cleaned),\n",
        "        \"rows_removed\": before - len(cleaned),\n",
        "        \"k\": k,\n",
        "        \"inclusive\": inclusive,\n",
        "        \"bounds\": bounds,\n",
        "    }\n",
        "    return cleaned, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "xyYf5jk-BpLD",
        "outputId": "3b48cad6-1a5e-4932-c126-bd6e819c1902"
      },
      "outputs": [],
      "source": [
        "# Assumes you already have:\n",
        "# - df (your full dataset)\n",
        "# - target_columns = [\"accommodates\",\"bathrooms\",\"bedrooms\",\"beds\",\"minimum_nights\",\n",
        "#                     \"maximum_nights\",\"number_of_reviews\",\"price\"]\n",
        "# - rio_listings = df[target_columns].copy()   # your prepared subset\n",
        "\n",
        "# Copy the dataset to work on IQR filtering\n",
        "#rio_iqr_input = rio_listings[target_columns].copy()\n",
        "rio_iqr_input = rio_listings.copy()\n",
        "# Remove outliers with default Tukey rule (k=1.5) and inclusive bounds\n",
        "rio_iqr, summary = remove_outliers_iqr(\n",
        "    df=rio_iqr_input,\n",
        "    columns=target_columns,\n",
        "    k=1.5,\n",
        "    inclusive=True,\n",
        "    dropna=True,\n",
        ")\n",
        "\n",
        "print(f\"Rows before:  {summary['rows_in']}\")\n",
        "print(f\"Rows after:   {summary['rows_out']}\")\n",
        "print(f\"Removed:      {summary['rows_removed']}\")\n",
        "print(\"Per-column bounds (low/up):\")\n",
        "for col, b in summary[\"bounds\"].items():\n",
        "    print(f\"  - {col}: [{b['low']:.3f}, {b['up']:.3f}]  (Q1={b['Q1']:.3f}, Q3={b['Q3']:.3f}, IQR={b['IQR']:.3f})\")\n",
        "\n",
        "rio_iqr.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "79RHVgYMC2RN",
        "outputId": "632e6e55-c117-420f-beed-0227c9b5dcb2"
      },
      "outputs": [],
      "source": [
        "rio_iqr.price.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CrIp4gmEXa5"
      },
      "source": [
        "## Correlation Heatmap of Rio Listings Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svPJAWd-EbRy",
        "outputId": "c8a05b46-f155-4bd9-ca65-07304fade5f1"
      },
      "outputs": [],
      "source": [
        "%pip install matplotlib seaborn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute pairwise correlation matrix using Pearson's correlation coefficient\n",
        "corr_matrix = rio_iqr.corr(method=\"pearson\")\n",
        "\n",
        "# Display features sorted by correlation strength with the target ('price')\n",
        "# This helps identify which features are most relevant to predict 'price'\n",
        "price_corr = corr_matrix[\"price\"].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation of features with target 'price':\\n\")\n",
        "print(price_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "vJPrSQQBEkpQ",
        "outputId": "0b1e75eb-83dd-46c9-ed20-8e0ecfee41b7"
      },
      "outputs": [],
      "source": [
        "# Visualize correlation matrix\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,          # show correlation values\n",
        "    fmt=\".2f\",           # format with 2 decimals\n",
        "    cmap=\"coolwarm\",     # colormap: negative=blue, positive=red\n",
        "    center=0,            # center color scale at 0\n",
        "    square=True,         # square cells\n",
        "    cbar_kws={\"shrink\": 0.75}  # adjust colorbar size\n",
        ")\n",
        "\n",
        "# Add a title to the heatmap\n",
        "plt.title(\"Correlation Heatmap of Rio Listings Features\", fontsize=16, pad=15)\n",
        "\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbWxbxSAKiuY"
      },
      "source": [
        "# End-to-End Solution Pipeline in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBpy8qJfLIFL"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yA2HcQvxKm9I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h38OikK_-d4M"
      },
      "source": [
        "## Architecture class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tYxCEnws8T7L"
      },
      "outputs": [],
      "source": [
        "class Architecture(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "\n",
        "        # We start by storing the arguments as attributes\n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model,\n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and\n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename,weights_only=False)\n",
        "\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3JfHlegLfIZ"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72eZwvEUe_pZ",
        "outputId": "04c9a168-65a8-45c6-aaf3-329ff5ff74eb"
      },
      "outputs": [],
      "source": [
        "rio_iqr.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hX2UppXlLizB"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 1) Extract features (X) and target (y) from the DataFrame\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Keep all numeric feature columns except the target 'price'\n",
        "feature_cols = [c for c in rio_iqr.columns if c != \"price\"]\n",
        "target_col   = \"price\"\n",
        "\n",
        "# Convert to NumPy arrays (float32 is ideal for PyTorch)\n",
        "X = rio_iqr[feature_cols].to_numpy(dtype=np.float32)     # shape (N, D)\n",
        "y = rio_iqr[target_col].to_numpy(dtype=np.float32).reshape(-1, 1)  # shape (N, 1)\n",
        "\n",
        "# Quick sanity checks\n",
        "assert not np.isnan(X).any(), \"Found NaNs in X. Clean/impute before training.\"\n",
        "assert not np.isnan(y).any(), \"Found NaNs in y. Clean/impute before training.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ywVHFf1cMynn"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 2) Build tensors BEFORE splitting (as you requested)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(13)\n",
        "\n",
        "x_tensor = torch.as_tensor(X).float()   # (N, D)\n",
        "y_tensor = torch.as_tensor(y).float()   # (N, 1)\n",
        "\n",
        "# Whole dataset\n",
        "dataset = TensorDataset(x_tensor, y_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4hfBXJG1M62r"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 3) Train/validation split using PyTorch's random_split\n",
        "# ---------------------------------------------------------------------\n",
        "ratio = 0.8\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * ratio)\n",
        "n_val   = n_total - n_train\n",
        "\n",
        "train_data, val_data = random_split(dataset, [n_train, n_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "ynzRFjUig8Ny",
        "outputId": "b176f3c1-693c-4a55-f2ea-326cc577c7e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "KlIa0mH3g_jD",
        "outputId": "1142981e-4445-4eea-ea99-4e7e642372ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "t-UaFv27hOyk"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Z-score without leakage\n",
        "# ---------------------------------------------------------------------\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# 0) Get split indices produced by random_split (already done above)\n",
        "train_idx = train_data.indices\n",
        "val_idx   = val_data.indices\n",
        "\n",
        "# 1) Compute mean/std ONLY on training subset\n",
        "eps = 1e-8\n",
        "mu  = x_tensor[train_idx].mean(dim=0)\n",
        "std = x_tensor[train_idx].std(dim=0, unbiased=False)\n",
        "std = torch.where(std < eps, torch.ones_like(std), std)  # avoid divide-by-zero\n",
        "\n",
        "y_mu  = y_tensor[train_idx].mean(dim=0)\n",
        "y_std = y_tensor[train_idx].std(dim=0, unbiased=False)\n",
        "y_std = torch.where(y_std < eps, torch.ones_like(y_std), y_std)\n",
        "\n",
        "\n",
        "# 2) Apply z-score to ALL features using training stats\n",
        "x_tensor_z = (x_tensor - mu) / std\n",
        "y_tensor_z = (y_tensor - y_mu) / y_std\n",
        "\n",
        "\n",
        "# 3) Rebuild dataset with normalized features and REUSE the SAME indices\n",
        "dataset_z  = TensorDataset(x_tensor_z, y_tensor_z)\n",
        "train_data = Subset(dataset_z, train_idx)\n",
        "val_data   = Subset(dataset_z, val_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENoqXNPICtLk"
      },
      "source": [
        "# Modificação da normalização para a escala Min-Máx usando Gemini\n",
        "Prompt:\"modifique o codigo de normalização para a escala Min-Máx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6wjaCQz-O-N"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Min-Max normalization without leakage\n",
        "# ---------------------------------------------------------------------\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# 0) Get split indices produced by random_split (assuming this was done previously)\n",
        "# If not, you would need to perform the split here or in a preceding cell\n",
        "train_idx = train_data.indices\n",
        "val_idx   = val_data.indices\n",
        "\n",
        "# 1) Compute min/max ONLY on training subset\n",
        "x_min = x_tensor[train_idx].min(dim=0).values\n",
        "x_max = x_tensor[train_idx].max(dim=0).values\n",
        "\n",
        "y_min = y_tensor[train_idx].min(dim=0).values\n",
        "y_max = y_tensor[train_idx].max(dim=0).values\n",
        "\n",
        "# Avoid division by zero if min and max are the same\n",
        "eps = 1e-8\n",
        "x_range = x_max - x_min\n",
        "x_range = torch.where(x_range < eps, torch.ones_like(x_range), x_range)\n",
        "\n",
        "y_range = y_max - y_min\n",
        "y_range = torch.where(y_range < eps, torch.ones_like(y_range), y_range)\n",
        "\n",
        "\n",
        "# 2) Apply min-max normalization to ALL features using training stats\n",
        "x_tensor_norm = (x_tensor - x_min) / x_range\n",
        "y_tensor_norm = (y_tensor - y_min) / y_range\n",
        "\n",
        "\n",
        "# 3) Rebuild dataset with normalized features and REUSE the SAME indices\n",
        "dataset_norm  = TensorDataset(x_tensor_norm, y_tensor_norm)\n",
        "train_data = Subset(dataset_norm, train_idx)\n",
        "val_data   = Subset(dataset_norm, val_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMQ-g3mbhhTe",
        "outputId": "b2df499c-478c-4089-c66f-52361a653f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batch X: torch.Size([16, 8]) | y: torch.Size([16, 1])\n"
          ]
        }
      ],
      "source": [
        "# 4) Recreate the DataLoaders\n",
        "batch_size  = 16\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(dataset=val_data,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Quick peek to confirm shapes\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(f\"Train batch X: {xb.shape} | y: {yb.shape}\")  # e.g., (16, D) and (16, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vJ_mpPZOxr1"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "doAjgsk-hyah"
      },
      "outputs": [],
      "source": [
        "# 5) (Optional but safer) Make model input dimension dynamic\n",
        "# Sets learning rate\n",
        "lr = 0.001\n",
        "torch.manual_seed(42)\n",
        "D = x_tensor.shape[1]  # number of features\n",
        "model = nn.Sequential(nn.Linear(D, 1))\n",
        "loss_fn = nn.MSELoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NZOZ67eOU8-2"
      },
      "outputs": [],
      "source": [
        "# executar para a usar o otimizador SGD\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "W1EAJ7YXVJqM"
      },
      "outputs": [],
      "source": [
        "# executar para usar o otimizador Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LTmEX20O-kk"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "FcJU8N3sO7D_"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "arch = Architecture(model, loss_fn, optimizer)\n",
        "arch.set_seed(42)\n",
        "arch.set_loaders(train_loader, val_loader)\n",
        "arch.train(n_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "xss7f44jPLjW",
        "outputId": "1e62e5a9-e6e9-4440-fa58-458328b8eab8"
      },
      "outputs": [],
      "source": [
        "fig = arch.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUC92XzdmOU9",
        "outputId": "492b10fd-bbc4-462c-9503-4dc118e60e3f"
      },
      "outputs": [],
      "source": [
        "rio_iqr.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEqX-fqriJuQ",
        "outputId": "782349d6-6068-44cb-dd8d-c0bb52006ee4"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------\n",
        "# Suppose you trained and still have mu, std (for X) and y_mu, y_std (for y)\n",
        "# -------------------------------------------------------------\n",
        "# Example: one new sample with 7 features (same order as feature_cols)\n",
        "X_new = np.array([[2.0, 1.0, 2.0, 2.0, 4.0, 5.0, 50.0, 30.0]], dtype=np.float32)\n",
        "# (this is just a made-up input)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1) Apply the SAME feature normalization (z-score using training mu/std)\n",
        "# -------------------------------------------------------------\n",
        "X_new_t = torch.as_tensor(X_new)\n",
        "X_new_norm = (X_new_t - mu) / std  # mu, std from TRAIN only\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Predict with your trained Architecture\n",
        "# -------------------------------------------------------------\n",
        "y_pred_z = arch.predict(X_new_norm.numpy())   # prediction in standardized space of y\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3) Revert target normalization back to original units\n",
        "# -------------------------------------------------------------\n",
        "y_pred_real = y_pred_z * y_std.item() + y_mu.item()\n",
        "\n",
        "# Convert to scalar\n",
        "y_pred_real_value = float(y_pred_real.squeeze())\n",
        "\n",
        "print(f\"Predicted price: R$ {y_pred_real_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2WoWHaCB3fu",
        "outputId": "f98fd286-70cc-4f49-ff4a-a16264da10e4"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------\n",
        "# Suppose you trained and still have x_min, x_range, y_min, y_range (for Min-Max)\n",
        "# -------------------------------------------------------------\n",
        "# Example: one new sample with 7 features (same order as feature_cols)\n",
        "X_new = np.array([[2.0, 1.0, 2.0, 2.0, 4.0, 5.0, 50.0, 30.0]], dtype=np.float32)\n",
        "# (this is just a made-up input)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1) Apply the SAME feature normalization (Min-Max using training min/max)\n",
        "# -------------------------------------------------------------\n",
        "X_new_t = torch.as_tensor(X_new)\n",
        "X_new_norm = (X_new_t - x_min) / x_range  # x_min, x_range from TRAIN only (Min-Max)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Predict with your trained Architecture\n",
        "# -------------------------------------------------------------\n",
        "y_pred_norm = arch.predict(X_new_norm.numpy())   # prediction in normalized space\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3) Revert target normalization back to original units\n",
        "# -------------------------------------------------------------\n",
        "y_pred_real = y_pred_norm * y_range.item() + y_min.item() # y_min, y_range from TRAIN only (Min-Max)\n",
        "\n",
        "\n",
        "# Convert to scalar\n",
        "y_pred_real_value = float(y_pred_real.squeeze())\n",
        "\n",
        "print(f\"Predicted price: R$ {y_pred_real_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hb2mgawUFx5T"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------\n",
        "# Evaluate the model performance on the validation set\n",
        "# -------------------------------------------------------------\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Get the normalized validation data\n",
        "val_x_tensor_norm = torch.as_tensor(val_data[:][0]).float()\n",
        "val_y_tensor_norm = torch.as_tensor(val_data[:][1]).float()\n",
        "\n",
        "# Predict on the validation set (in normalized space)\n",
        "val_y_pred_norm = arch.predict(val_x_tensor_norm.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "X8gUJuf5WKpi"
      },
      "outputs": [],
      "source": [
        "# Assuming Z-score normalization was used\n",
        "val_y_true_real = val_y_tensor_norm.numpy() * y_std.item() + y_mu.item()\n",
        "val_y_pred_real = val_y_pred_norm * y_std.item() + y_mu.item()\n",
        "normalization_method = \"Z-score\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPDfpn_aWa9I"
      },
      "outputs": [],
      "source": [
        "# Assuming Min-Max normalization was used\n",
        "val_y_true_real = val_y_tensor_norm.numpy() * y_range.item() + y_min.item()\n",
        "val_y_pred_real = val_y_pred_norm * y_range.item() + y_min.item()\n",
        "normalization_method = \"Min-Max\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND89SP_pWqE_",
        "outputId": "6f1c0440-684d-4e1b-ed8a-685721aaf407"
      },
      "outputs": [],
      "source": [
        "if normalization_method != \"Unknown\":\n",
        "    # Calculate MSE and RMSE on the real scale\n",
        "    mse = mean_squared_error(val_y_true_real, val_y_pred_real)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Calculate MAE and R-squared on the real scale\n",
        "    mae = mean_absolute_error(val_y_true_real, val_y_pred_real)\n",
        "    r2 = r2_score(val_y_true_real, val_y_pred_real)\n",
        "\n",
        "    print(f\"Evaluation Metrics ({normalization_method} Normalization):\")\n",
        "    print(f\"Mean Squared Error (MSE) on validation set: {mse:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE) on validation set: {rmse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE) on validation set: {mae:.2f}\")\n",
        "    print(f\"R-squared (R2) on validation set: {r2:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usando o Lazy Predict para avaliação de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install ipywidgets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lazypredict\n",
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "# Use the training and validation data from previous steps\n",
        "X_train = train_data[:][0].numpy()\n",
        "y_train = train_data[:][1].numpy()\n",
        "X_test = val_data[:][0].numpy()\n",
        "y_test = val_data[:][1].numpy()\n",
        "\n",
        "\n",
        "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
        "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
